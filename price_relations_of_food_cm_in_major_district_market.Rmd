---
title: Price relations of food commodities in regional markets of Nepal
author: "Samita Paudel"
date: "6/28/2019"
output: 
  bookdown::pdf_document2:
    latex_engine: xelatex
    keep_tex: true
    toc: false
    includes:
      in_header: template_header.tex
  word_document:
    # reference_docx: draft_word_template.docx
fontsize: 12pt
geometry: margin=1in
citecolor: DodgerBlue4
linestretch: 1
bibliography: bibliographies.bib
link-citations: yes
---

```{r setup, include=FALSE}
require(AER)
require(tidyverse)
require(forecast)
require(tsibble)
require(ggfortify)
require(fable)
require(fabletools)
require(tseries)
require(sf)
require(units)
# require(timetk)
# require(timeSeries)
# require(timeDate)
require(urca)
# require(rvest)
# require(rebus)
require(lubridate)
theme_set(theme_bw())

knitr::opts_chunk$set(tidy = FALSE, echo = FALSE, cache = TRUE, message = F, warning = F)
options(htmltools.dir.version = FALSE, 
        # knitr.table.format = "latex",
        kableExtra.latex.load_packages = FALSE)
```


```{r handy-functions}
# named group split (copied from: https://github.com/tidyverse/dplyr/issues/4223)
named_group_split <- function(.tbl, ...) {
  grouped <- group_by(.tbl, ...)
  names <- rlang::eval_bare(rlang::expr(paste(!!!group_keys(grouped), sep = " / ")))

  grouped %>% 
    group_split() %>% 
    rlang::set_names(names)
}

```

# Administrative summary of Nepal

```{r administrative-summary-nepal}
# require(rvest)
# require(htmltools)
# 
# wiki_district <- "https://en.wikipedia.org/wiki/List_of_districts_of_Nepal" %>% 
#   read_html()
# 
# wiki_district %>% 
#   html_nodes("table") %>% 
#   .[6:13] %>%
#   map(~html_table(.x, fill = T) %>% 
#         as_tibble(.name_repair = "minimal")) %>% 
#   map2(.y = paste("./data/nepal_provincial/", 
#                   c("province_and_hq", paste("province", 1:7, sep = "")), 
#                   ".csv", sep = ""), 
#        .f = ~write_csv(x = .x, path = , .y, na = ""))

##  read from saved csvs
nepal_provincial <- map(list.files("./data/nepal_provincial", pattern = ".*\\d", full.names = T), ~read_csv(.x)) %>% 
  map(~.x %>% slice(1:(nrow(.x)-1))) %>% 
  map_dfr(c, .id = "province")

# nepal_provincial %>% 
#   mutate(Name = str_trim(str_remove_all(Name, "District"))) %>% 
#   dplyr::select(province, district_name = Name) %>% 
#   mutate(province = paste0("Province ", province))
```

## Study districts and market centres

A map of study districts.

<!-- Shape file is available in `./data/nepal_provincial` directory. -->

```{r study-districts, fig.cap="Geographical perspective on market selections for study", fig.width=8, out.width="95%"}
# read nepal district map
np_dist <- sf::st_read("./data/nepal_provincial/NPL_districts_poly_sd_171123.shp", quiet = T)
np_province <- sf::st_read("./data/nepal_provincial/Nepal_Province.shp", quiet = T)

np_mhub_districts <- enframe(c("Morang", "Parsa", "Kathmandu", "Kaski", "Rupandehi", "Banke", "Kailali"), 
                             value = "district", name = "study") %>% 
  mutate(study = TRUE)

## mutate np_mhub_districts record sheet to match district names;
# so, yes only uppercasing will do the job
np_mhub_districts <- np_mhub_districts %>% 
  mutate(district = str_to_upper(district))

# ggplot2 plotting
np_dist <- np_dist %>% 
  left_join(np_mhub_districts, by = c("DISTRICT" = "district"))

# transform crs of district map to EPSG "4326" (same as that of province map)
np_dist <- sf::st_transform(np_dist, crs = "+proj=longlat +datum=WGS84 +no_defs")

# seven provinces
seven_province <- np_province$State_Name

# ggplot2 plotting
seven_districts_gg <- ggplot() +
  geom_sf(data = st_geometry(np_province), alpha = 1, lwd = 0.4, color = "blue") + 
  geom_sf(data = st_geometry(np_dist), aes(fill = np_dist$study), lwd = 0.3, alpha = 1) + 
  # geom_sf(data = st_geometry(np_districts_production), 
  #         alpha = 0.2, lwd = 0.5, color = "blue", aes(fill = "wheat3")) +
  scale_fill_manual(values = c("wheat4", "green"), labels = c("Yes", "No")) +
  labs(fill = "Study performed", 
       title = "Districts of which market study was performed") +
  ggrepel::geom_label_repel(data=(np_dist[!is.na(np_dist$study), ] %>% 
                             mutate(lon=map_dbl(.$geometry, ~st_centroid(.x)[[1]]), # add centroid values for labels
                                    lat=map_dbl(.$geometry, ~st_centroid(.x)[[2]]))), 
                           aes(x=lon, y=lat, label=DISTRICT), size = 2) + # default size is big
  guides(fill=FALSE) + # remove legend
  xlab(NULL) + ylab(NULL) +
  theme_bw() + 
  theme(panel.grid.major = element_line(colour = "wheat2")) # change the grid (graticule) color
seven_districts_gg

# ggsave("./outputs/nepal_province_and_geocentres_map.png", 
#        plot = eight_gg, units = "in", dpi = 300, width = 10)

```


# Retail price of rice and wheat in major districtwise Nepalese market hubs

The price series of districtwise market hubs available for study is mostly imbalanced and irregular and contains data for following 21 districts.

Achham, Banke, Bhojpur, Chitwan, Dhankuta, Dhanusha, Doti, Illam, Jhapa, Jumla, Kailali, Kaski, Kathmandu, Morang, Nuwakot, Palpa, Parsa, Ramechap, Rolpa, Rupandehi, Surkhet.

```{r food-retail-import}

# import data
retail_pr_np_wfp <- read_csv("./data/prices_nepal/wfp_food_prices_nepal_markethubwise_nepal_2005-2019.csv", skip = 1) %>% 
  # convenient date type to work with
  mutate(date = yearmonth(date)) %>% 
  # deselect unnecessary columns
  select(-last_col(), -country, -currency, -adm1id, -catid, -cmid, -mktid, -ptid, -umid, -sn, -unit) %>% 
  # include only food items (filter out fuel)
  filter(category != 'non-food') %>%
  as_tsibble(index = date, key = c(cmname, mktname)) %>% 
  # just to remove bloating columns, perfrom group_by and summarize
  group_by(cmname, admname, mktname) %>%
  summarise(price = mean(price, na.rm = T), 
            lprice = log(price)) %>%
  ungroup()

# subset the data for some districtwise market series only
retail_mhub <- retail_pr_np_wfp %>% 
  filter(mktname %in% c("Morang", "Parsa", "Kathmandu", "Kaski", "Rupandehi", "Banke", "Kailali"))

```


<!-- ```{r} -->
<!-- retail_mhub %>%  -->
<!--   as_tibble() %>%  -->
<!--   count(cmname, year = year(date)) %>%  -->
<!--   mutate(break_grp = c(rep(1, times = floor(nrow(.)/2)), rep(2, times = nrow(.)-floor(nrow(.)/2)))) %>%  -->
<!--   group_split(break_grp) %>%  -->
<!--   bind_cols() %>%  -->
<!--   select(-starts_with("break")) %>%  -->
<!--   knitr::kable(caption = "Number of yearly records in the series available for study",  -->
<!--                col.names = rep(colnames(.)[1:(ncol(.)/2)], times = 2), booktabs = T) -->

<!-- ``` -->

## Aggregate series summary

Joint time series plot of rice and wheat retail prices aggregated over selected districtwise markets.

```{r aggregated-price-summary}
retail_mhub %>% 
  skimr::skim_to_list() %>% 
  walk(~skimr::pander(.x))
  
retail_mhub %>% 
  group_by(cmname) %>% 
  summarise(price_mean = mean(price, na.rm = T)) %>% 
  ungroup() %>% 
  ggplot(aes(x = `date`, y = `price_mean`, color = `cmname`)) +
  geom_line(size = 1.1) +
  labs(x = "Date", y = "Mean price (Rs)") +
  ggtitle(label = "Mean price of food commodities", 
          subtitle = "Aggregated over selected districtwise markethubs")

```


```{r early-exploration}
# retail_acf <- retail_mhub %>%
#   group_split(cmname, mktname, ) %>%
#   map(~.x %>% fill_gaps(.full = FALSE)) %>%
#   map(~.x %>% tidyr::fill(price, .direction = "down")) %>%
#   map(~tsbox::ts_ts(.x)) %>%
#   map(~.x %>% na.omit()) %>%
#   map(~acf(.x, plot = F))
# # data is highly autocorrelated, hence it needs differencing to remove the trend.

```

Time series plot of retail price of rice in Kathmandu market. Series has some time gaps at random periods (shown on line plot on the lower right). Similarly, autocorrelation of series for various lag, with first order difference, is presented in the lower left.

```{r retail-kathmandu-series-exploratory}

# single time series exploration
retail_mhub %>% 
  filter(mktname == "Kathmandu", 
         cmname == "Rice - Retail") %>% 
  fill_gaps() %>% 
  feasts::gg_tsdisplay(y = price)

# tidy acf
retail_acf <- retail_mhub %>% 
  group_by(cmname, mktname) %>% 
  nest() %>% 
  mutate(retail_acf = map(data, ~acf(.x$price, plot = FALSE)))
# broom::tidy methods available for cleaning coefficients

# retail_acf %>%
#   .[["retail_acf"]] %>%
#   .[[1]] %>%
#   autoplot()

# # feasts::ACF() is analogous to stats::acf for autocorrelation diagnostics
# retail_mhub %>% 
#   fill_gaps() %>% 
#   feasts::ACF(price) %>% 
#   autoplot() # plots on all series

# tidy pacf
retail_pacf <- retail_mhub %>% 
  group_by(cmname, mktname) %>% 
  nest() %>% 
  mutate(retail_pacf = map(data, ~pacf(.x$price, plot = FALSE)))
# broom::tidy methods available for cleaning coefficients

# retail_pacf %>% 
#   .[["retail_pacf"]] %>% 
#   .[[1]] %>% 
#   autoplot()

# # feasts::PACF() is analogous to stats::pacf
# retail_mhub %>% 
#   fill_gaps() %>% 
#   feasts::PACF(price) %>% 
#   autoplot() # plots on all series
```

A possible measure to removing non-stationary trend in the series is by differencing (with `diff`) . However, before progressing we confirm that justifiable lag operations can infact render the series free of trends. For this at two fundamentally different unit tests are performed -- Augmented Dickey-Fueller test and KPSS test.

While the series needs detrending in order to perform regression, we also have to consider the time gaps in the available dataset.

```{r time-gaps}
# although it is non obvious, there still are several gaps (implicit missing) in the series
# visualize time gaps in price (missing)
retail_mhub %>%
  # has_gaps() # presents which key combinations has gaps
  # scan_gaps() # shows exactly which date intervals are missing
  count_gaps() %>% # useful to locate which time period has how many missing entries
  ggplot(aes(x = mktname, colour = mktname)) +
  geom_linerange(aes(ymin = .from, ymax = .to)) +
  geom_point(aes(y = .from)) +
  geom_point(aes(y = .to)) +
  coord_flip() +
  theme(legend.position = "bottom")

```

All district market series presented after time gap filling. First order lag is used to fill the missing entries in the series. Missing values for each series are imputed independent of other series.

```{r time-gap-filling}
# fill time gaps in price (missing)
retail_mhub_filled <- retail_mhub %>%
  # fill_gaps(lprice = lag(lprice), .full = FALSE) # doesn't work
  fill_gaps(.full = FALSE) %>% 
  tidyr::fill(lprice, .direction = "down") %>% 
  mutate(lprice_diff = difference(lprice, lag = 1, differences = 1), 
         lprice_diff12 = difference(lprice, lag = 12, differences = 1))

retail_mhub_filled %>%
  ggplot(aes(x = date, y = lprice)) +
  geom_line() +
  facet_wrap(~mktname + cmname, nrow = 3) +
  ggtitle("Augmented log(price) of series", 
          subtitle = "Time gaps filled with first order lags")
```


# Unit root testing

## Unit root (ADF and KPSS) test of retail price

The ADF, available in the function `adf.test()` (in the package `tseries`) implements the t-test of $H_0: \gamma = 0$ in the regression, below.

$$
  \Delta {{Y}_{t}}={{\beta
  }_{1}}+{{\beta }_{2}}t+\gamma {{Y}_{t-1}}+ \sum\limits_{i=1}^{m}{\delta_i \Delta
    {{Y}_{t-i}}+{{\varepsilon }_{t}}}
$$
  
The null is therefore that x has a unit root. If only x has a non-unit root, then the x is stationary (rejection of null hypothesis).

```{r adf-kpss-test-retail, results='asis'}
# price series are log transformed to stabilize the variance
retail_mhub <- retail_mhub %>% 
  mutate(lprice = log(price))

# adf test for major citywise series
stationary_series_mhub <- retail_mhub %>% 
  as_tibble() %>% # doesn't work with tsibble
  group_by(cmname, mktname) %>% 
  summarise(adf_lprice = list(tseries::adf.test(lprice)), 
            kpss_lprice = list(tseries::kpss.test(lprice)),
              # pp_lprice = list(tseries::pp.test(log(price), type = "Z(t_alpha)")), # Phillips-Perron test
            n_observations = n()) %>% 
  mutate(adf_lprice_pvalue = map_dbl(adf_lprice, ~.x[["p.value"]]), 
         adf_lprice_tstatistic = map_dbl(adf_lprice, ~.x[['statistic']]), 
         adf_stationary = adf_lprice_pvalue < 0.05) %>% 
  mutate(kpss_lprice_pvalue = map_dbl(kpss_lprice, ~.x[["p.value"]]), 
         kpss_lprice_tstatistic = map_dbl(kpss_lprice, ~.x[['statistic']]), 
         kpss_stationary = kpss_lprice_pvalue > 0.05)

stationary_series_mhub_unit_root_table <- stationary_series_mhub %>% 
  select(-adf_lprice, -kpss_lprice) %>% 
  named_group_split(cmname) %>% 
  map2(.y = c("Rice", "Wheat"), 
       .f = ~.x %>% 
         select(-cmname, -n_observations) %>% 
         rename_all(function(x)str_replace_all(x, "_", " ")) %>% 
         rename_all(function(x)str_replace_all(x, "lprice", "")) %>% 
         rename_all(function(x)str_squish(x)) %>% 
         knitr::kable(caption = paste("Unit root test of district market retail log(price) data of", .y), booktabs = TRUE, digits = 2) %>% 
         kableExtra::column_spec(1:7, width = rep("4em", 7)))

walk(stationary_series_mhub_unit_root_table, print)

# # stationary series plot
# # none are stationary!, based on test result of 'kpss'
# # some are stationary, however, based on adf test alone.
# stationary_series_mhub_gg <- stationary_series_mhub %>%
#   filter(adf_stationary & kpss_stationary) %>% 
#   dplyr::select(mktname, cmname) %>%
#   left_join(retail_pr_np_wfp) %>%
#   ggplot(aes(x = date, y = log(price))) +
#   geom_line() +
#   facet_wrap(~mktname + cmname, nrow = 3) +
#   ggtitle("Price (log transformed) of stationary series")
# 
# stationary_series_mhub_gg
# # ggsave("./output/unit_tests_mhub_stationary.png",
# #        stationary_series_mhub_gg, width = 8, height = 6, units = "in")

```

The ADF test was parametrized with the alternative hypothesis of stationarity. This extends to following assumption in the model parameters;

$$
-2 \leq \gamma \leq 0\ \text{or } (-1 < 1+\phi < 1)
$$

`k` in the function refers to the number of $\delta$ lags, i.e., $1, 2, 3, ...., m$ in the model equation. 

The number of lags `k` defaults to `trunc((length(x)-1)^(1/3))`, where `x` is the series being tested. The default value of `k` corresponds to the suggested upper bound on the rate at which the number of lags, `k`, should be made to grow with the sample size for the general ARMA(p,q) setup `citation(package = "tseries")`.

For a Dickey-Fueller test, so only up to AR(1) time dependency in our stationary process, we set `k = 0`. Hence we have no $\delta$s (lags) in our test.

The DF model can be written as:

$$
Y_t = \beta_1 + \beta_2 t + \phi Y_{t-1} + \varepsilon_t
$$

It can be re-written so we can do a linear regression of $\Delta Y_t$ against $t$ and $Y_{t-1}$ and test if $\phi$ is different from 0. If only, $\phi$ is not zero and assumption above ($-1 < 1+\phi < 1$) holds, the process is stationary. If $\phi$ is straight up 0, then we have a random walk process -- all white noise.

$$
\Delta {Y}_{t}=\beta_1+\beta_2 t+\gamma {Y}_{t-1} + \varepsilon_{t}
$$

Alternative to above discussed tests, the Phillips-Perron test with its nonparametric correction for autocorrelation (essentially employing a HAC estimate of the long-run variance in a Dickey-Fuller-type test instead of parametric decorrelation) may be used. It is available in the function `pp.test()`.

## Unit root test based lag order differencing determination

An alternative to decomposition for removing trends is differencing [@woodward2017applied]. We define the difference operator as,

\begin{equation}
\nabla x_t = x_t - x_{t-1},
\label{eqn:difference-operator}
\end{equation}

and, more generally, for order $d$

\begin{equation}
\nabla^d x_t = (1-\mathbf{B})^d x_t,
\label{eqn:order-d-difference-operator}
\end{equation}

Where $\mathbf{B}$ is the backshift operator (i.e., $\mathbf{B}^k x_t = x_{t-k}$ for $k \geq 1$).

Applying the difference to a random walk, the most simple and widely used time series model, will yield a time series of Gaussian white noise errors $\{w_t\}$:

\begin{equation}
  \begin{aligned}
    \nabla (x_t &= x_{t-1} + w_t) \\
    x_t - x_{t-1} &= x_{t-1} - x_{t-1} + w_t \\
    x_t - x_{t-1} &= w_t
  \end{aligned}
  \label{eqn:random-walk-series}
\end{equation}


```{r lag-diff-determine}

# list of market univariate series
retail_mhub_ts_lst <- retail_mhub %>% 
  named_group_split(cmname, mktname) %>%
  map(~.x %>% fill_gaps(.full = FALSE)) %>%
  map(~.x %>% tidyr::fill(price, .direction = "down")) %>%
  map(~tsbox::ts_ts(select(.x, -lprice))) %>%
  map(~.x %>% na.omit())

# # optimum number of differing required is given by `ndiffs`.
# # difference the series and plot
# map(retail_mhub_ts_lst, ~diff(.x, lag = ndiffs(.x, test = "kpss")))
# map(retail_mhub_ts_lst, ~diff(.x, lag = 1))
# map(retail_mhub_ts_lst, ~diff(.x, lag = 12))

# "kpss" test shows shows that all series need differencing! i.e., they are all non-stationary 
# `type` of test does not affect the results.
```

Differencing is required for *all* series to make them stationary, as inferred by the `ndiffs` function which employed popular unit root tests. A detailed presentation of the test routines is given below. We describe in detail the ADF test, while only tabluation of summary statistics of `kpss` test is made herein.

## Unit root tests of first order differenced series

All major districtwise market series series are non-stationary, meaning that they have a trend associated with time.

We test the logged prices of the series after first order differencing. Here we perform a more conservative Dickey-Fueller, instead of Augmented DF, test.

```{r adf-test-diffretail}

# # first order differencing and logging of prices
stationary_series_mhub_lprice_diff <- retail_mhub %>% 
  group_by(cmname, mktname) %>% # group in order to perform grouped differencing
  mutate(lprice_diff = difference(log(price), lag = 1, differences = 1)) %>% 
  na.omit() %>% # makes a tibble
  summarise(df_lprice_diff = list(tseries::adf.test(lprice_diff, k = 0)), 
            kpss_lprice_diff = list(tseries::kpss.test(lprice_diff)),
              # pp_lprice_diff = list(tseries::pp.test(lprice_diff, type = "Z(t_alpha)")), # Phillips-Perron test
            n_observations = n()) %>% 
  mutate(df_lprice_diff_pvalue = map_dbl(df_lprice_diff, ~.x[["p.value"]]), 
         df_lprice_diff_tstatistic = map_dbl(df_lprice_diff, ~.x[['statistic']]), 
         df_stationary = df_lprice_diff_pvalue < 0.05) %>% 
  mutate(kpss_lprice_diff_pvalue = map_dbl(kpss_lprice_diff, ~.x[["p.value"]]), 
         kpss_lprice_diff_tstatistic = map_dbl(kpss_lprice_diff, ~.x[['statistic']]), 
         kpss_stationary = kpss_lprice_diff_pvalue > 0.05)

# # plot of differenced series aggregated for district markets
# retail_mhub_filled %>% 
#   ggplot(aes(x = date, y = lprice_diff, color = cmname)) +
#   geom_line() +
#   labs(x = "Date", y = "p(1) differenced log(price) series") +
#   theme(legend.position = "bottom")

# all series are stationary
stationary_series_mhub_lprice_diff %>%
  filter(df_stationary & kpss_stationary) %>% 
  dplyr::select(mktname, cmname) %>%
  left_join(retail_pr_np_wfp) %>%
  mutate(lprice_diff = difference(log(price), lag = 1, differences = 1)) %>% 
  ggplot(aes(x = date, y = `lprice_diff`)) +
  geom_line() +
  facet_wrap(~mktname + cmname, nrow = 3) +
  ggtitle("First lag differenced stationary series of log(price)")

```

The first order differencing renders all series stationary.

## ARIMA modeling log(retail) price

### Model definition

An AR(p) model

$$
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t - p} + \varepsilon_t
$$

For, AR(1) model, following assumptions may hold.

- When $\phi_1 = 0$, $y_t$ is equivalent to white noise;
- When $\phi_1 = 1$ and $c = 0$, $y_t$ is equivalent to random walk;
- When $\phi_1 = 1$ and $c \neq 0$, $y_t$ is equivalent to a random walk with drift;
- When $\phi_1 < 0$, $y_t$ tends to oscillate between positive and negative value.

```{r simulating-ar-process}
## simulating AR processes

# generate a white noise process
Y <- rnorm(250, 0, 2)
par(mfrow = c(1, 2))
plot.ts(Y)
acf(Y)

# generate a random walk (phi = 1)
Y <- rnorm(250, 0, 2)
Y2 <- numeric(250)
Y2[1] <- Y[1]
for(i in 2:250) Y2[i] <- Y2[i - 1] + Y[i]
par(mfrow = c(1, 2))
plot.ts(Y2)
acf(Y2)

# generate an AR(1) with phi = -0.5, oscillating series
Y <- rnorm(250, 0, 2)
Y3 <- numeric(250)
Y3[1] <- Y[1]
for(i in 2:250) Y3[i] <- -0.5 * Y3[i - 1] + Y[i]
par(mfrow = c(1, 2))
plot.ts(Y3)
acf(Y3)

# generate and AR(1) with phi = 0.5, side then side (groups of positive and negative acfs)
Y <- rnorm(250, 0, 2)
Y4 <- numeric(250)
Y4[1] <- Y[1]
for(i in 2:250) Y4[i] <- 0.5 * Y4[i - 1] + Y[i]
par(mfrow = c(1, 2))
plot.ts(Y4)
acf(Y4)

# generate an AR(2) series
Y5 <- arima.sim(n = 250, list(ar = c(-0.5, 0.3), sd = 10))
# here the list is the model holding coefficients for AR/MA/ARMA; here phi1 = -0.5, and phi2 = 0.3.
par(mfrow = c(1, 2))
plot.ts(Y5)
acf(Y5)

```

Why and AR(2) process is stationary

```{r}
phi1 <- seq(from = -2.5, to = 2.5, length = 51)

plot(phi1, 1+phi1, lty = "dashed", type = "l", xlab = "", 
     ylab = "", cex.axis = .8, ylim = c(-1.5, 1.5))

abline(a = -1, b = 0, lty = "dashed")
abline(a = 1, b = -1, lty = "dashed")

title(ylab = expression(phi[2]), 
      xlab = expression(phi[1]), cex.lab = .8)
polygon(x = phi1[6:46], y = 1-abs(phi1[6:46]), col = "gray")
lines(phi1, -phi1^2/4)
text(0, -0.5, expression(phi[2]<phi1[2]^2/4), cex = .7)
text(1.2, 0.5, expression(phi[2]>1-phi1[1]), cex = .7)
text(-1.75, .5, expression(phi1[2]>1+phi[1]), cex = .7)

```


An MA(q) process is

$$
y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q}
$$

Following assumptions may hold for a MA(q) model

- For an MA(1) model: $-1 < \theta_1 < 1$
- For an MA(2) model: $-1 < \theta_2 < 1$, $\theta_2 + \theta_1 > -1$, $\theta_1 - \theta_2 < 1$

```{r simulating-ma-process}
# generate MA(1) with theta 0.5
Y1 <- arima.sim(model = list(ma = 0.5), n = 200)
plot(Y1)
acf(Y1)

# generate MA(1) with theta1 = -0.5
Y2 <- arima.sim(model = list(ma = -0.5), n = 200)
plot(Y2)
acf(Y2)

# generate MA(2)
Y3 <- arima.sim(model = list(ma = c(0.5, 03)), n = 200)
plot(Y3)
acf(Y3)

# generate MA(2)
Y4 <- arima.sim(model = list(ma = c(0.5, -0.3)), n = 200)
plot(Y4)
acf(Y4)

# generate MA(2)
Y5 <- arima.sim(model = list(ma = c(-0.5, 0.3)), n = 200)
plot(Y5)
acf(Y5)
```


A process $x_t$ is said to be ARIMA(p, d, q) if

$$
\label{eqn:arima}
\begin{aligned}
  \nabla^d x_t = ( 1- B)^d x_t
\end{aligned}
$$
if ARMA(p, q). In general, we write the total model as:

$$
\label{eqn:arima-total}
\phi (B)(1-B)^d x_t = \theta (B)w_t
$$

If $E (\nabla^d x_t) = \mu$, we write the model as:

$$
\label{eqn:arima-full}
\phi(B)(1-B)^d x_t = \delta + \theta (B) w_t
$$

Where $\delta = \mu (1-\phi_1 - ... - \phi_p )$.

Specification of an ARIMA model;

```{r arima-fitting}
# # stats::arima modeling
# # banke district retail log(price)
# retail_mhub_ts_lst[[1]] %>% 
#   auto.arima() %>% 
#   summary()

# tidy ARIMA modeling
retail_mhub_arima_fit <- retail_mhub_filled %>% 
  fabletools::model(
    cmad_arima = fable::ARIMA(lprice, 
                 unitroot_spec = unitroot_options(ndiffs_alpha = 0.05,
                                                  nsdiffs_alpha = 0.05,
                                                  ndiffs_pvalue = ~feasts::unitroot_kpss(.)["kpss_pvalue"],
                                                  nsdiffs_pvalue = ~feasts::feat_stl(., .period)[2] < 0.64))
  )
```


<!-- ```{r single-arima-summary} -->
<!-- # single model reporting -->
<!-- retail_mhub_arima_fit_ktm <- retail_mhub_arima_fit %>% -->
<!--   filter(mktname == "Kathmandu") %>% -->
<!--   filter(cmname == "Rice - Retail") -->

<!-- retail_mhub_arima_fit_ktm %>% -->
<!--   select(cmad_arima) %>% -->
<!--   report() # single model summary -->

<!-- retail_mhub_arima_fit_ktm %>% -->
<!--   select(cmad_arima) %>% -->
<!--   coef() # same as broom::tidy() -->

<!-- ## interpreting  -->
<!-- retail_mhub_arima_fit_ktm %>%  -->
<!--   select(cmad_arima) %>%  -->
<!--   pull("cmad_arima") %>%  -->
<!--   .[[1]] %>% -->
<!--   broom::tidy() %>%  -->
<!--   knitr::kable(caption = "Model estimates for a fitted ARIMA model", booktabs = TRUE) %>%  -->
<!--   kableExtra::add_footnote(label = "Model estimates show log(price) series for kathmandu show an MA(2) process, an ARIMA(0,1,2) model, with drift") -->

<!-- # to remove insignificant arima coefficients specify arima function as -->
<!-- # arima(x, order = c(p, d, q), fixed = c(....)) -->
<!-- # fixed refers to fixed coefficients. it is a vector that is total of (p + q) length, in order  -->

<!-- # In fable, you can get prediction intervals from the forecast object using hilo() and in plots using autoplot(). -->

<!-- # forecasts can be autoplotted -->
<!-- retail_mhub_arima_fit %>% -->
<!--   filter(mktname == "Kathmandu") %>% -->
<!--   filter(cmname == "Rice - Retail") %>% -->
<!--   fabletools::forecast(h = 5) %>% -->
<!--   autoplot(retail_mhub, level = NULL) -->
<!-- ``` -->

```{r multiple-arima-summary, results='asis'}

# model summary
retail_mhub_arima_fit_summary_tab <- retail_mhub_arima_fit %>%
  select(cmad_arima) %>%
  # report() # single model summary
  broom::glance() %>% 
  select(-ar_roots, -ma_roots, -.model, -admname) %>% 
  named_group_split(cmname) %>% 
  map2(.y = names(.), 
       .f = ~.x %>% 
        select(-cmname) %>% 
        knitr::kable(caption = paste("ARIMA model summary for multiple log(price) series of major districtwise market hubs of", .y), booktabs = T, digits = 3))

walk(retail_mhub_arima_fit_summary_tab, print)

# full model statistics
retail_mhub_arima_fit_coef_tab <- retail_mhub_arima_fit %>%
  select(cmad_arima) %>%
  # report() # single model summary
  broom::tidy() %>% 
  select(-admname, -.model) %>% 
  named_group_split(cmname) %>% 
  map2(.y = names(.), 
       .f = ~.x %>% 
         select(-cmname) %>% 
         knitr::kable(caption = paste("Model coefficients of ARIMA model for multiple log(price) series of major districtwise market hubs of", .y), booktabs = T, digits = 3))

walk(retail_mhub_arima_fit_coef_tab, print)
```

### Seasonality, trend and moving (average) filters

Default filter used is "convolution" (moving average type), specified through `method` argument. Alternative is "recursive" (autoregression type).

```{r trend-fit-lprice}
retail_kathmandu_ts <- retail_mhub_filled %>% 
  select(-admname) %>% 
  filter(mktname == "Kathmandu") %>% 
  named_group_split(cmname) %>% 
  map(~.x %>% tsbox::ts_ts())

# two season-trend decomposition
par(mfrow = c(2, 1))
plot(retail_kathmandu_ts[[1]][,1], ylab = "price")
lines(timeSeries::filter(retail_kathmandu_ts[[1]][,1], c(1/2, rep(1, 11), 1/2)/12), col = 2) # or stats::filter
plot(retail_kathmandu_ts[[1]][,2], ylab = "lprice")
lines(timeSeries::filter(retail_kathmandu_ts[[1]][,2], c(1/2, rep(1, 11), 1/2)/12), col = 2) # or stats::filter
```

Smoothed annual mean and standard deviation of retail price/lprice/diff_lprice series in Kathmandu.

```{r mean-and-sd-year}
# rolling standard deviation (shows how much is the variation)
layout(matrix(1:4, ncol = 1), widths = 1, heights = c(3,3,3,3.5), respect = FALSE)
par(mar = c(1, 4, 0.5, 2))
plot(rollapply(retail_kathmandu_ts[[1]][,1], 12, sd), ylab = "price", xaxt = "n", xlab = NULL) # note the high variation around 1980s.
par(mar = c(1, 4, 0.5, 2))
plot(rollapply(retail_kathmandu_ts[[1]][,2], 12, sd), ylab = "lprice", xaxt = "n", xlab = NULL)
par(mar = c(1, 4, 0.5, 2))
plot(rollapply(retail_kathmandu_ts[[1]][,3], 12, sd), ylab = "diff_lprice", xaxt = "n", xlab = NULL)
par(mar = c(3, 4, 0.5, 2))
plot(rollapply(retail_kathmandu_ts[[1]][,4], 12, sd), ylab = "diff_lprice12")

# rolling mean
# detrending using rolling function
layout(matrix(1:4, ncol = 1), widths = 1, heights = c(3,3,3,3.5), respect = FALSE)
par(mar = c(1, 4, 0.5, 2))
plot(rollapply(retail_kathmandu_ts[[1]][,1], 12, mean), ylab = "price", xaxt = "n", xlab = NULL) # note the high variation around 1980s.
par(mar = c(1, 4, 0.5, 2))
plot(rollapply(retail_kathmandu_ts[[1]][,2], 12, mean), ylab = "lprice", xaxt = "n", xlab = NULL)
par(mar = c(1, 4, 0.5, 2))
plot(rollapply(retail_kathmandu_ts[[1]][,3], 12, mean), ylab = "diff_lprice", xaxt = "n", xlab = NULL)
par(mar = c(3, 4, 0.5, 2))
plot(rollapply(retail_kathmandu_ts[[1]][,4], 12, mean), ylab = "diff_lprice12")

```


## VAR modeling

VAR is a system regression model, i.e., there are more than one dependent variable. The regression is defined by a set of linear dynamic equations where each variable is specified as a function of an equal number of lags of itself and all other variables in the system. Any additional variable, adds to the modeling complexity by increasing an extra equation to be estimated.

The vector autoregression (VAR) model extends the idea of univariate autoregression to $k$ time series regressions, where the lagged values of *all* $k$ series appear as regressors. Put differently, in a VAR model we regress a *vector* of time series variables on lagged vectors of these variables. As for AR($p$) models, the lag order is denoted by $p$ so the VAR($p$) model of two variables $X_t$ and $Y_t$ ($k=2$) is given by the equations:

$$
\begin{aligned}
  Y_t =& \, \beta_{10} + \beta_{11} Y_{t-1} + \dots + \beta_{1p} Y_{t-p} + \gamma_{11} X_{t-1} + \dots + \gamma_{1p} X_{t-p} + u_{1t}, \\
  X_t =& \, \beta_{20} + \beta_{21} Y_{t-1} + \dots + \beta_{2p} Y_{t-p} + \gamma_{21} X_{t-1} + \dots + \gamma_{2p} X_{t-p} + u_{2t}.
\end{aligned}
$$

The $\beta$s and $\gamma$s can be estimated using OLS on each equation.

It is straightforward to estimate VAR models in `R`. A feasible approach is to simply use `lm()` for estimation of the individual equations. Furthermore, the `vars` package provides standard tools for estimation, diagnostic testing and prediction using this type of models.

Only when the assumptions presented below hold, the OLS estimators of the VAR coefficients are consistent and jointly normal in large samples so that the usual inferential methods such as confidence intervals and $t$-statistics can be used [@metcalfe2009introductory].

Two series {w_{x,t}} and {w_{y,t}} are bivariate white noise if they are stationary and their cross-covariances $\gamma_{xy}(k) = Cov(w_{x,t}, w_{y, t+k})$ satisfies

$$
\gamma_{xx}(k) = \gamma_{yy}(k) = \gamma_{xy}(k) = 0\ \text{for all } k \neq 0
$$

Two time series ${x_t}$ and ${y_t}$, follow a vector autoregressive process of order 1 (denoted VAR(1)) if,

\begin{equation}
\label{eqn:var-equations}
\begin{aligned}
x_t &= \theta_{11} x_{t-1} + \theta_{12} y_{t-1} + w_{x,t} \\
y_t &= \theta_{21} x_{t-1} + \theta_{22} y_{t-1} + w_{y,t}
\end{aligned}
\end{equation}

where, {$w_{x,t}$} and {$w_{y,t}$} are bivariate white noise and $\theta_ij$ are model parameters. If the white noise sequences are defined with mean 0 and the process is stationary, both time series {x_t} and {y_t} have a zero mean. The simplest way of incorporating a mean is to define {x_t} and {y_t} as deviations from mean values. 

Equation \@ref(eqn:var-equations) can be rewritten in matrix notation as:

$$
Z_t = \phi Z_{t-1} + w_t
$$

The parameters of a var(p) model can be estimated using the `ar` function in `R`, which
selects a best-fitting order p based on the smallest information criterion values. 

<!-- ```{r var-simulation} -->
<!-- # simulated bivariate white noise process of ... and the parameters from the stationary VAR(1) model: -->

<!-- library(mvtnorm) -->
<!-- cov.mat <- matrix(c(1, 0.8, 0.8, 1), nr = 2) -->
<!-- w <- rmvnorm(1000, sigma = cov.mat) -->
<!-- cov(w) -->

<!-- wx <- w[, 1] -->
<!-- wy <- w[, 2] -->
<!-- ccf(wx, wy, main = "") -->

<!-- par_var1_m1 <- matrix(c(0.4, 0.2, 0.3, 0.1), nrow = 2) -->

<!-- par_var1_m1 -->

<!-- # The absolute value of roots of the equation is given by: -->

<!-- determinant_var1 <- function(x) { -->
<!--   # x A matrix of parameter \phi -->
<!--   c(1, -(x[1,1] + x[2,2]), -(-x[1,1]*x[2,2] + x[1,2]*x[2,1])) -->
<!-- } -->

<!-- Mod(polyroot(determinant_var1(par_var1_m1))) -->

<!-- # a VAR(1) process is simulated below and the parameters from the simulated series estimated using ar. -->

<!-- x <- y <- rep(0, 1000) -->

<!-- x[1] <- wx[1] -->
<!-- y[1] <- wy[1] -->

<!-- for (i in 2:1000) { -->
<!--   x[i] <- par_var1_m1[1,1] * x[i-1] + par_var1_m1[1,2]*y[i-1] + wx[i] -->
<!--   y[i] <- par_var1_m1[2,1] * x[i-1] + par_var1_m1[2,2]*y[i-1] + wy[i] -->
<!-- } -->

<!-- xy.ar <- ar(cbind(x, y)) -->
<!-- xy.ar$ar[, , ] -->

<!-- # As expected, the parameter estimates are close to the underlying model values. If the simulation is repeated many times with different realizations of the bivariate white noise, the sampling distribution of the esimators of the parameters in the model can be approximated by the histograms of the estimates together with the correlations between estimates. This is the principle used to construct bootstrap confidence intervals for model parameters when they have been estimated from time series. -->
<!-- ``` -->

The structure of VARs also allows to jointly test restrictions across multiple equations. For instance, it may be of interest to test whether the coefficients on all regressors of the lag $p$ are zero. This corresponds to testing the null that the lag order $p-1$ is correct. Large sample joint normality of the coefficient estimates is convenient because it implies that we may simply use an $F$-test for this testing problem. The explicit formula for such a test statistic is rather complicated but fortunately such computations are easily done using the `ttcode("R")` functions we work with in this chapter. Just as in the case of a single equation, for a multiple equation model we choose the specification which has the smallest $BIC(p)$, where 

$$
\begin{aligned}
  BIC(p) =& \, \log\left[\text{det}(\widehat{\Sigma}_u)\right] + k(kp+1) \frac{\log(T)}{T}.
\end{aligned}
$$

with $\widehat{\Sigma}_u$ denoting the estimate of the $k \times k$ covariance matrix of the VAR errors and $\text{det}(\cdot)$ denotes the determinant. 

As for univariate distributed lag models, one should think carefully about variables to include in a VAR, as adding unrelated variables reduces the forecast accuracy by increasing the estimation error. This is particularly important because the number of parameters to be estimated grows qudratically to the number of variables modeled by the VAR. In the application below we shall see that economic theory and empirical evidence are helpful for this decision.

<!-- ```{r retail-var-fitt-oldstyle} -->

<!-- # # # Multivariate VAR (with 7 variables, i.e., markets) with lag order 1 -->
<!-- # rice -->
<!-- retail_mhub_var_rice <- retail_mhub_filled %>%  -->
<!--   select(cmname:lprice, -price, -admname) %>% -->
<!--   filter(cmname == "Rice - Retail") %>%  -->
<!--   tsbox::ts_ts() %>%  -->
<!--   na.omit() %>%  -->
<!--   vars::VAR(p = 1, ic = "SC", type = "const", lag.max = 2) -->

<!-- # wheat -->
<!-- retail_mhub_var_wheat <- retail_mhub_filled %>%  -->
<!--   select(cmname:lprice, -price, -admname) %>% -->
<!--   filter(cmname == "Wheat - Retail") %>%  -->
<!--   tsbox::ts_ts() %>%  -->
<!--   na.omit() %>%  -->
<!--   vars::VAR(p = 1, ic = "SC", type = "const", lag.max = 2) -->
<!-- ``` -->

<!-- ```{r retail-var-summary-old-style} -->

<!-- # rice summary -->
<!-- retail_mhub_var_rice %>% -->
<!--   summary() -->
<!-- # districtwise residual vs fit plots with acf and pacf -->
<!-- # plot(retail_mhub_var_rice) -->

<!-- # wheat summary -->
<!-- retail_mhub_var_wheat %>% -->
<!--   summary() -->
<!-- # districtwise residual vs fit plot with acf and pacf -->
<!-- # plot(retail_mhub_var_wheat) -->

<!-- # # # model dissection -->
<!-- # each var model is a list of linear model objects -->

<!-- # inspect individual model and check terms for significant effects -->
<!-- retail_mhub_var_rice$varresult$Rice...Retail_Kathmandu %>% broom::tidy() -->
<!-- retail_mhub_var_rice$varresult$Rice...Retail_Kathmandu %>% coefplot::coefplot() -->
<!-- # this suggests that retail price of rice in kathmandu are strongly associated with its retail price in the same district during previous month. -->

<!-- retail_mhub_var_rice$varresult$Rice...Retail_Parsa %>% coefplot::coefplot() -->
<!-- retail_mhub_var_rice$varresult$Rice...Retail_Morang %>% coefplot::coefplot() -->
<!-- retail_mhub_var_rice$varresult$Rice...Retail_Kailali %>% coefplot::coefplot() -->
<!-- retail_mhub_var_rice$varresult$Rice...Retail_Banke %>% coefplot::coefplot() -->
<!-- retail_mhub_var_rice$varresult$Rice...Retail_Kaski %>% coefplot::coefplot() -->
<!-- retail_mhub_var_rice$varresult$Rice...Retail_Rupandehi %>% coefplot::coefplot() -->
<!-- ``` -->


```{r retail-var-fit-tidy, results='asis'}
# # # # tidy VAR model fitting  
retail_mhub_var_fit <- retail_mhub_filled %>%
  model(var_fit = VAR(vars(lprice) ~ AR(p = 1))) %>% # fit all series with AR1 model
  named_group_split(cmname, mktname) %>% 
  map(~.x %>% select(-admname, -mktname, -cmname)) %>% 
  map_dfr(bind_rows, .id = "cmname-mktname")
  
map(retail_mhub_var_fit$var_fit, ~.x %>% glance()) %>% 
  map_dfr(bind_rows) %>% 
  bind_cols(select(retail_mhub_var_fit, `cmname-mktname`)) %>% 
  select(`cmname-mktname`, everything()) %>% 
  unnest(sigma2) %>% 
  knitr::kable(caption = "Model performance indicators of VAR(AR(1)) model for selected districtwise market hub series in Rice and Wheat.", booktabs = TRUE, longtable = TRUE, digits = 3)
  

retail_mhub_var_fit_coef <- retail_mhub_var_fit %>% 
  .[["var_fit"]] %>% 
  map(~.x %>% tidy()) %>% 
  set_names(retail_mhub_var_fit$`cmname-mktname`) %>% 
  map_dfr(~bind_rows(.x), .id = "cmname-mktname") %>% 
  separate(col = `cmname-mktname`, into = c("cmname", "mktname"), sep = " / ") %>% 
  named_group_split(cmname) %>% 
  map(~.x %>% select(-cmname, -.response)) %>% 
  map2(.y = names(.), 
       .f = ~.x %>% 
         knitr::kable(caption = paste("Model coefficients of VAR model for multiple log(price) series of major districtwise market hubs of", .y), booktabs = T, longtable = TRUE, digits = 3))

walk(retail_mhub_var_fit_coef, print)

# # multimodel summary and forecast
# retail_mhub_var_fit$var_fit %>% 
#   forecast() %>% 
#   autoplot()
```

## Differenced series

```{r differenced-series-viz}
# price series "x"
retail_pr_np_wfp %>% 
  filter(admname == "Central") %>% 
  filter(cmname == "Wheat - Retail") %>% 
  tsibble::fill_gaps(.full = F) %>% 
  ggplot(aes(x = date, y = price, color = mktname)) +
  geom_line(size = 1.2) +
  labs(x = "Date") +
  theme(legend.position = "bottom")

# logged price series "lx"
retail_pr_np_wfp %>% 
  filter(admname == "Central") %>% 
  filter(cmname == "Wheat - Retail") %>% 
  tsibble::fill_gaps(.full = F) %>% 
  mutate_at("price", list(~log(.))) %>% 
  ggplot(aes(x = date, y = price, color = mktname)) +
  geom_line(size = 1.2) +
  labs(x = "Date", y = "log(price)") +
  theme(legend.position = "bottom")

# first lag differenced logged price series "dlx"
retail_pr_np_wfp %>% 
  filter(admname == "Central") %>% 
  filter(cmname == "Wheat - Retail") %>% 
  tsibble::fill_gaps(.full = F) %>% 
  # tidyr::fill(price, .direction = "down") %>% 
  mutate_at("price", list(~log(.))) %>% 
  mutate_at("price", list(~difference(.))) %>% 
  ggplot(aes(x = date, y = price, color = mktname)) +
  geom_line(size = 1.2) +
  labs(x = "Date", y = "dlog(price)") +
  theme(legend.position = "bottom")

# twelveth lag differenced, first lag differenced logged price series "ddlx"
# price series "x"
retail_pr_np_wfp %>% 
  filter(admname == "Central") %>% 
  filter(cmname == "Wheat - Retail") %>% 
  tsibble::fill_gaps(.full = F) %>% 
  # tidyr::fill(price, .direction = "down") %>% 
  mutate_at("price", list(~log(.))) %>% 
  mutate_at("price", list(~difference(difference(.), lag = 12))) %>% 
  ggplot(aes(x = date, y = price, color = mktname)) +
  geom_line(size = 1.2) +
  labs(x = "Date", y = "dd12log(price)") +
  theme(legend.position = "bottom")
```

```{r sarima-demo}
# # var model with arima lag p(12)
# retail_mhub_filled %>%
#   model(var_fit = VAR(vars(lprice) ~ AR(p = 12))) %>% # fit all series with AR1 model
#   .[["var_fit"]] %>% 
#   .[[1]] %>% 
#   broom::tidy()

```

<!-- ## Tidy VAR fitting -->

<!-- ```{r} -->
<!-- cbind(mdeaths, fdeaths) %>% as_tsibble(pivot_longer = F) %>% model(VAR(vars(log(mdeaths), fdeaths)~AR(3))) %>% report() -->
<!-- ``` -->

<!-- # Annual wholesale price of major food commodities -->

<!-- Annual average wholesale price of Barley, Buckwheat, Maize, Millet, Rice/paddy and Wheat since 1991 through 2017 (27 years). -->

<!-- ```{r food-wholesale, results='asis'} -->
<!-- food_wholesale_np <- readxl::read_xlsx("./data/prices_nepal/cereal_wholesale_nepal.xlsx", skip = 1) %>%  -->
<!--   mutate(year = `Year Code`) %>%  -->
<!--   dplyr::select(-`Year Code`, -SN) %>%  -->
<!--   as_tsibble(index = `year`, key = `Item`) -->

<!-- food_wholesale_np_table <- food_wholesale_np %>% -->
<!--   as_tibble() %>%  -->
<!--   na.omit() %>% -->
<!--   # count(Item) %>% # this is balanced for number of years -->
<!--   mutate(break_grp = as.numeric(cut_number(1:NROW(.), n = 3))) %>%  -->
<!--   group_split(break_grp) %>%  -->
<!--   # bind_cols() # because number of rows is not multiple -->
<!--   map(~select(.x, -starts_with("break"))) %>%  -->
<!--   map(~.x %>% knitr::kable(caption = "Wholesale price data of major food commodities of Nepal since 1991", booktabs = T, longtable = TRUE)) -->

<!-- walk(food_wholesale_np_table, print) -->
<!-- ``` -->

# Import historical rice and wheat data

Retail price of various rice commodities and wheat flour since 1976 AD.

```{r rice-wheat-historical-annual-series}

cmname_file <- list.files(path = "./data", pattern = "npl_nepalindata_", full.names = T)
cmname_list <- map(cmname_file, ~read_csv(.x, skip = 1))

historical_pr_rice_wheat <- cmname_list %>% 
  map(~(t(.x)[-1,])) %>% 
  map_dfr(~as_tibble(.x, rownames = "year", .name_repair = ~ c("cmname", "price"))) %>% 
  mutate(year = yearmonth(year), 
         price = as.numeric(price))

historical_pr_rice_wheat %>% 
  ggplot(aes(x = year, y = price, color = cmname)) +
  geom_line() +
  scale_color_viridis_d() +
  # facet_grid(~cmname)
  guides(color = guide_legend(label.position = "left", direction = "vertical", 
                              title.hjust = 0.5, title = "Commodity",
                              ncol = 1, size = 8, keywidth = 1, keyheight = 1))

```


# Cointegration

## Residual based

Since the food commodities are spatially linked, more of so because they occupy the same domestic market, it is obvious that factor affecting price of one inevitably affects other, especially that of same crop in a nearby market. Having evidence for nonstationarity, it is of interest to test for a common nonstationary component by means of a cointegration test (Non-stationarity is more valid for development regionwise price series).

A two step method proposed by @hylleberg1990seasonal can be used to test for cointegration.

The procedure simply regressess one series on the other and performs a unit root test on the residuals. This test is often named after @phillips1990asymptotic. Specifically, `po.test()` performs a Phillips-Perron test using an auxiliary regression without a constant and linear trend and the Newey-West estimator for the required long-run variance.

The test computes the Phillips-Ouliaris test for the null hypothesis that series is not cointegrated [@R-tseries].

We check the rice retail price series for all combination major districtwise markets.

```{r series-mktname-wise-cname}
retail_mhub_cmwise <- retail_pr_np_wfp %>% 
  group_by(cmname, mktname) %>% 
  summarise(lprice = log(mean(price, na.rm = T))) %>% 
  ungroup() %>% 
  group_split(cmname) %>% 
  set_names(c("rice", "wheat")) %>% 
  map(~fill_gaps(.x, .full = FALSE) %>% 
        tidyr::fill(lprice, .direction = "down"))
```


```{r pairwise-phillips-cointegration}
# need to perform pairwise cointegration tests
# philip-oularis test
# we map to simultaneously test in different commodities (rice and wheat)
pairwise_series_name <- combn(c("Morang", "Parsa", "Kathmandu", "Kaski", "Rupandehi", "Banke", "Kailali"), 2) %>% 
  t()

# rice series
rice_series_combination <- map2(pairwise_series_name[,1],
     pairwise_series_name[,2],
    ~tsbox::ts_ts(filter(retail_mhub_cmwise[[1]], mktname %in% c(.x, .y))))

# # po test results aren't symmetric for same series done in different order
rice_series_combination_po_test <- map(rice_series_combination, ~tseries::po.test(.x[, 1:2])) %>% 
  set_names(unite(as_tibble(pairwise_series_name), col = "single_col", sep = "-")$`single_col`)

# wheat series
wheat_series_combination <- map2(pairwise_series_name[,1],
     pairwise_series_name[,2],
    ~tsbox::ts_ts(filter(retail_mhub_cmwise[[2]], mktname %in% c(.x, .y))))

# # po test results aren't symmetric for same series done in different order
wheat_series_combination_po_test <- map(wheat_series_combination, ~tseries::po.test(.x[, 1:2])) %>% 
  set_names(unite(as_tibble(pairwise_series_name), col = "single_col", sep = "-")$`single_col`)

rice_cointegration_pair <- rice_series_combination_po_test %>% map_df(~ list(p_value = .x[["p.value"]], 
                                                  statistic = .x[["statistic"]]), .id = "combination")
wheat_cointegration_pair <- wheat_series_combination_po_test %>% map_df(~ list(p_value = .x[["p.value"]], 
                                                  statistic = .x[["statistic"]]), .id = "combination")

rice_cointegration_pair %>%
  knitr::kable(caption = "Phillips-Ouliaris cointegration test for Rice log(price) series of selected district markethubs", booktabs = T, longtable = T, digits = 3)

wheat_cointegration_pair %>% 
  knitr::kable(caption = "Phillips-Ouliaris cointegration test for Wheat log(price) series of selected district markethubs", booktabs = T, longtable = T, digits = 3)
```

Note `po.test` does not handle missing values, so we fix them through imputation. It is implemented through `tidyr::fill(..., .direction = "down")`.

The test suggests that all series (Both that of wheat and rice) are cointegrated for selected pairwise combination of district markets.

The problem with this approach is that it treats both series in an asymmetric fashion, while the concept of cointegration demands that the treatment be symmetric.

The po.test() function is testing the cointegration with Phillip’s Z_alpha test, which is the second residual-based test described by @phillips1990asymptotic. Because the po.test() will use the series at the first position to derive the residual used in the test, results would be determined by the series on the most left-hand side^[https://www.r-craft.org/r-news/phillips-ouliaris-test-for-cointegration/].

The Phillips-Ouliaris test implemented in the `ca.po()` function from the urca package is different. In the `ca.po()` function, there are two cointegration tests implemented, namely “Pu” and “Pz” tests. Although both the `ca.po()` function and the po.test() function are supposed to do the Phillips-Ouliaris test，outcomes from both functions are completely different.

Similar to Phillip’s Z_alpha test, the Pu test also is not invariant to the position of each series and therefore would give different outcomes based upon the series on the most left-hand side. On the contrary, the multivariate trace statistic of Pz test has its appeal in that the outcome won’t change by the position of each series.

## VAR based

The standard tests proceeding in a symmetric manner stem from Johansen's full-information maximum likelihood approach [@johansen1991estimation].

A general vector autoregressive model is similar to the AR(p) model except that each quantity is vector valued and matrices are used as the coefficients. The general form of the VAR(p) model, without drift, is given by:

$$
\label{eqn:var-general}
{\bf y_t} = {\bf \mu} + A_1 {\bf y_{t-1}} + \ldots + A_j {\bf y_{t-j}} + {\bf \varepsilon_t} 
$$

Where ${\bf \mu}$ is the vector-valued mean of the series, $A_i$ are the coefficient matrices for each lag and ${\bf \varepsilon_t}$ is a multivariate Gaussian noise term with mean zero.

At this stage we can form a Vector Error Correction Model (VECM) by differencing the series (Equation \@ref(eqn:vecm-differenced)).

$$
\label{eqn:vecm-differenced}
\Delta {\bf y_t} = {\bf \mu} + A {\bf y_{t-1}} + \Gamma_1 \Delta {\bf y_{t-1}} + \ldots + \Gamma_j \Delta {\bf y_{t-j}} + {\bf \varepsilon_t} 
$$

Where $\Delta {\bf y_t} = {\bf y_t} - {\bf y_{t-1}}$ is the differencing operator, $A$ is the coefficient matrix for the first lag and $\Gamma_i$ are the matrices for each differenced lag.

For a $p^{th}$ -order cointegrated vector autoregressive (VAR) model, the error correction form is (omitting deterministic components; both no intercept or trend in either cointegrating equation or test var), we may rewrite the VAR in the form of Equation \@ref(eqn:johansens) [@johansen1991estimation].

<!-- For more information refer to:  -->
<!-- - [Eviews documentation](./literatures/johansen_cointegration_eviews.pdf)  -->
<!-- - [Quantstart post](./literatures/johansen_cointegration_quantstart.pdf) -->
<!-- - [Kevin kotze's post](./literatures/johansen_cointegration_kevin_kotze.pdf) -->

<!-- equation labeling trick: https://stackoverflow.com/questions/55923290/consistent-math-equation-numbering-in-bookdown-across-pdf-docx-html-output -->

$$
\label{eqn:johansens}
\Delta y_t = \Pi y_{t-1} + \sum_{j = 1}^{p-1} {\Gamma_j \Delta y_{t-j}} + \varepsilon_t
$$

(Although, for simplicity sake, we assume absence of deterministic trends, there are five popular scenarios of including such trends in a cointegration test. All of these are described in [@johansen1995identifying].)

Where,

$$
\Pi = \sum^{p}_{i = 1}{A_{i}-I}; \Gamma = -\sum^{p}_{j = i + 1}{j}
$$

Granger's representation theorem asserts that if the coefficient matrix $\Pi$ has reduced rank $r < k$, then there exist $kxr$ matrices $\alpha$ and $\beta$ each with rank $k$ such that $\Pi = \alpha \beta^{\prime}$ and $\beta^{\prime}y_t$ is $I(0)$.

To achieve this an eigenvalue decomposition of $A$ is carried out. The rank of the matrix $A$ is given by $r$ and the Johansen test sequentially tests whether this rank $r$ is equal to zero, equal to one, through to $r=n-1$, where $n$ is the number of time series under test.

The null hypothesis of $r=0$ means that there is no cointegration at all. A rank $r > 0$ implies a cointegrating relationship between two or possibly more time series.

The eigenvalue decomposition results in a set of eigenvectors. The components of the largest eigenvector admits the important property of forming the coefficients of a linear combination of time series to produce a stationary portfolio. Notice how this differs from the CADF test (often known as the Engle-Granger procedure) where it is necessary to ascertain the linear combination a priori via linear regression and ordinary least squares (OLS).

In summary, the test checks for the situation of no cointegration, which occurs when the matrix $A=0$. So, starting with the base value of $r$ (i.e., $r=0$), if the test statistic is greater than critical values of at the 10%, 5% and 1% levels, this would imply that we are **able** to reject the null of no cointegration. For the case r<=1, we if the calculated test statistic is below the critical values of, we are **unable** to reject the null, and the number of cointegrating vectors is between 0 and 1. The relevant tests are available in the function `urca::ca.jo()`. The basic version considers the eigenvalues of the matrix $\Pi$ in the preceding equation.

Here, we employ the trace statistic -- the maximum eigenvalue, or "lambdamax" test is available as well -- in an equation amended by a constant term (specified by ecdet = "const"), yielding:

```{r cajo-test-procedure}

# # Johansens cointegration test
# # data
# retail_mhub_cmwise # (generated in code chunk 'series-mktname-wise-cname')
# pairwise_series_name # (generated in code chunk 'pairwise-phillips-cointegration')

# rice_series_combination # (generated in code chunk 'series-mktname-wise-cname')
# wheat_series_combination # (generated in code chunk 'series-mktname-wise-cname')

# # ca.jo test
rice_series_combination_cajo_test <- map(rice_series_combination, ~urca::ca.jo(log(.x), ecdet = "const", type = "trace")) %>% 
  set_names(unite(as_tibble(pairwise_series_name), col = "single_col", sep = "-")$`single_col`)

wheat_series_combination_cajo_test <- map(wheat_series_combination, ~urca::ca.jo(log(.x), ecdet = "const", type = "trace")) %>% 
  set_names(unite(as_tibble(pairwise_series_name), col = "single_col", sep = "-")$`single_col`)

```

Johansen cointegration test summary and time series plots for rice (district marketwise)

```{r rice-cajo-test, fig.width=6, out.width="90%", results='asis'}
rice_series_combination_cajo_test_tidy <- map(rice_series_combination_cajo_test, ~summary(.x)) %>% 
  # .[1] %>% 
  map(.f = function(x){
    map(.x = c("model", "type", "teststat", "cval", "lambda", "V", "W"), .f = function(y)slot(x, y))
    })


# test statistic and confidence interval summary of log(price)
rice_series_combination_cajo_test_statcval <- map(rice_series_combination_cajo_test_tidy, ~cbind(test_stat = .x[[3]], conf_val = .x[[4]]) %>% 
                                                     as_tibble(rownames = "gamma")) %>% 
  map2(.y = names(.), 
       .f = ~.x %>% 
         knitr::kable(caption = paste("Johansen cointegration test summary for", .y), booktabs = TRUE, longtable = TRUE))
walk(rice_series_combination_cajo_test_statcval, print)

# map(rice_series_combination_cajo_test, ~plot(.x))
```


Johansen cointegration test summary and time series plots for wheat (district marketwise)

```{r wheat-cajo-test, fig.width=6, out.width="90%", results='asis'}
wheat_series_combination_cajo_test_tidy <- map(wheat_series_combination_cajo_test, ~summary(.x)) %>% 
  # .[1] %>% 
  map(.f = function(x){
    map(.x = c("model", "type", "teststat", "cval", "lambda", "V", "W"), .f = function(y)slot(x, y))
    })

# test statistic and confidence interval summary of log(price)
wheat_series_combination_cajo_test_statcval <- map(wheat_series_combination_cajo_test_tidy, ~cbind(test_stat = .x[[3]], conf_val = .x[[4]]) %>% 
                                                     as_tibble(rownames = "gamma")) %>% 
  map2(.y = names(.), 
       .f = ~.x %>% 
         knitr::kable(caption = paste("Johansen cointegration test summary for", .y), booktabs = TRUE, longtable = TRUE))
walk(wheat_series_combination_cajo_test_statcval, print)
  
# map(wheat_series_combination_cajo_test, ~plot(.x, type = "single"))
```


```{r wheat-rice-ci-out-bind}

wheat_series_combination_cajo_test_ci_out <- map(wheat_series_combination_cajo_test, 
    ~.x@x %*% .x@V[-(.x@P + 1), ])
rice_series_combination_cajo_test_ci_out <- map(rice_series_combination_cajo_test, 
    ~.x@x %*% .x@V[-(.x@P + 1), ])

```


```{r grid-arrange-share-legend}
grid_arrange_share_legend_m <- function(..., nrow = 1, ncol = length(list(...)), position = c("bottom", "right")) {
  plots <- list(...)
  position <- match.arg(position)
  g <- ggplot2::ggplotGrob(plots[[1]] + ggplot2::theme(legend.position = position))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position = "none"))
  gl <- c(gl, nrow = nrow, ncol = ncol)
  
  combined <- switch(position,
                     "bottom" = gridExtra::arrangeGrob(do.call(gridExtra::arrangeGrob, gl),
                                                       legend,
                                                       ncol = 1,
                                                       heights = grid::unit.c(unit(1, "npc") - lheight, lheight)),
                     "right" = gridExtra::arrangeGrob(do.call(gridExtra::arrangeGrob, gl),
                                                      legend,
                                                      ncol = 2,
                                                      widths = grid::unit.c(unit(1, "npc") - lwidth, lwidth)))
  return(combined)
}
```

Wheat series and cointegration plots

```{r wheat-coint-plots, fig.width=10, fig.height=6, out.width="100%", fig.show='hold', fig.align='center'}
# first variable series
wheat_series_ci_df1 <- map2(wheat_series_combination_cajo_test, 
                            wheat_series_combination_cajo_test_ci_out,
                            ~.x@x[, 1] %>% 
                              as_tsibble() %>% 
                              mutate(ci_test = .y[, 1]))

wheat_series_ci_df1_bind <- wheat_series_ci_df1 %>% 
  map_dfr(c, .id = "which_pair") %>% 
  as_tsibble(index = "index", key = which_pair)

wheat_series_df1_bind_var_gg <- wheat_series_ci_df1_bind %>%
  ggplot(aes(x = index, y = value, color = which_pair)) +
  geom_line() +
  ggtitle("Pairs of time series plot of y1 variable") +
  guides(color = guide_legend(title = "Market region")) +
  labs(x = "Date", y = NULL)

wheat_series_ci_df1_bind_var_gg <- wheat_series_ci_df1_bind %>% 
  ggplot(aes(x = index, y = ci_test, color = which_pair)) +
  geom_line() +
  ggtitle("Pairs of cointegration relation of 1st variable") +
  guides(color = guide_legend(title = "Market region")) +
  labs(x = "Date", y = NULL)

# second variable series
wheat_series_ci_df2 <- map2(wheat_series_combination_cajo_test, 
                            wheat_series_combination_cajo_test_ci_out,
                            ~.x@x[, 2] %>% 
                              as_tsibble() %>% 
                              mutate(ci_test = .y[, 2]))

wheat_series_ci_df2_bind <- wheat_series_ci_df2 %>% 
  map_dfr(c, .id = "which_pair") %>% 
  as_tsibble(index = "index", key = which_pair)

wheat_series_df2_bind_var_gg <- wheat_series_ci_df2_bind %>%
  ggplot(aes(x = index, y = value, color = which_pair)) +
  geom_line() +
  ggtitle("Pairs of time series plot of y2 variable") +
  guides(color = guide_legend(title = "Market region")) +
  labs(x = "Date", y = NULL)

wheat_series_ci_df2_bind_var_gg <- wheat_series_ci_df2_bind %>% 
  ggplot(aes(x = index, y = ci_test, color = which_pair)) +
  geom_line() +
  # scale_color_discrete(name = "Market region") +
  ggtitle("Pairs of cointegration relation of 2nd variable") +
  guides(color = guide_legend(title = "Market region")) +
  labs(x = "Date", y = NULL)

wheat_lprice_series_gg <- grid_arrange_share_legend_m(wheat_series_df1_bind_var_gg, wheat_series_df2_bind_var_gg, ncol = 2, position = "bottom")
wheat_ci_series_gg <- grid_arrange_share_legend_m(wheat_series_ci_df1_bind_var_gg, wheat_series_ci_df2_bind_var_gg, ncol = 2, position = "bottom")

plot(wheat_lprice_series_gg)
plot(wheat_ci_series_gg)

# walk2(c("./output/wheat_lprice_series_gg.png", "./output/wheat_ci_series_gg.png"),
#      list(wheat_lprice_series_gg, wheat_ci_series_gg),
#      ~ggsave(.x, .y, device = "png", width = 12, height = 8, units = "in", dpi = 280))

```

Rice series and cointegration plots

```{r rice-coint-plots, fig.width=10, fig.height=6, out.width="100%", fig.show='hold', fig.align='center'}
# first variable series
rice_series_ci_df1 <- map2(rice_series_combination_cajo_test, 
                            rice_series_combination_cajo_test_ci_out,
                            ~.x@x[, 1] %>% 
                              as_tsibble() %>% 
                              mutate(ci_test = .y[, 1]))

rice_series_ci_df1_bind <- rice_series_ci_df1 %>% 
  map_dfr(c, .id = "which_pair") %>% 
  as_tsibble(index = "index", key = which_pair)

rice_series_df1_bind_var_gg <- rice_series_ci_df1_bind %>%
  ggplot(aes(x = index, y = value, color = which_pair)) +
  geom_line() +
  ggtitle("Pairs of time series plot of y1 variable") +
  guides(color = guide_legend(title = "Market region")) +
  labs(x = "Date", y = NULL)

rice_series_ci_df1_bind_var_gg <- rice_series_ci_df1_bind %>% 
  ggplot(aes(x = index, y = ci_test, color = which_pair)) +
  geom_line() +
  ggtitle("Pairs of cointegration relation of 1st variable") +
  guides(color = guide_legend(title = "Market region")) +
  labs(x = "Date", y = NULL)

# second variable series
rice_series_ci_df2 <- map2(rice_series_combination_cajo_test, 
                            rice_series_combination_cajo_test_ci_out,
                            ~.x@x[, 2] %>% 
                              as_tsibble() %>% 
                              mutate(ci_test = .y[, 2]))

rice_series_ci_df2_bind <- rice_series_ci_df2 %>% 
  map_dfr(c, .id = "which_pair") %>% 
  as_tsibble(index = "index", key = which_pair)

rice_series_df2_bind_var_gg <- rice_series_ci_df2_bind %>%
  ggplot(aes(x = index, y = value, color = which_pair)) +
  geom_line() +
  ggtitle("Pairs of time series plot of y2 variable") +
  guides(color = guide_legend(title = "Market region")) +
  labs(x = "Date", y = NULL)

rice_series_ci_df2_bind_var_gg <- rice_series_ci_df2_bind %>% 
  ggplot(aes(x = index, y = ci_test, color = which_pair)) +
  geom_line() +
  # scale_color_discrete(name = "Market region") +
  ggtitle("Pairs of cointegration relation of 2nd variable") +
  guides(color = guide_legend(title = "Market region")) +
  labs(x = "Date", y = NULL)

rice_lprice_series_gg <- grid_arrange_share_legend_m(rice_series_df1_bind_var_gg, rice_series_df2_bind_var_gg, ncol = 2, position = "bottom")
rice_ci_series_gg <- grid_arrange_share_legend_m(rice_series_ci_df1_bind_var_gg, rice_series_ci_df2_bind_var_gg, ncol = 2, position = "bottom")

plot(rice_lprice_series_gg)
plot(rice_ci_series_gg)

# walk2(c("./output/rice_lprice_series_gg.png", "./output/rice_ci_series_gg.png"),
#      list(rice_lprice_series_gg, rice_ci_series_gg),
#      ~ggsave(.x, .y, device = "png", width = 12, height = 8, units = "in", dpi = 280))

```


# Order of integration

In practice, "order of integration" provides you with the number of times you have to difference a series in order to obtain a covariance-stationary series.

The use of the term "integration" _does_ have something to do with the usual meaning of the term, but in its discrete incarnation (i.e. with "summation"). It comes from the fact that, looking "upstream", a series integrated of order $1$, $I(1)$, can be represented as the sum of the elements of a series integrated of order $0$:

Consider the stochastic process $\{X_t\}$, and assume that it is $I(0)$. Define the process;

$$
Z_t = \sum_{i=1}^tX_i
$$

Then

$$
\Delta Z_t = Z_t - Z_{t-1} = \sum_{i=1}^tX_i - \sum_{i=1}^{t-1}X_i = X_t
$$

So the process $\{\Delta Z_t\}$ is $I(0)$ and then the process $\{Z_t\}$ is $I(1)$, while also being the sum of the elements of $\{X_t\}$.  

And this can continue for higher orders of integration, as you can easily check.

<!-- *Question* -->

<!-- First puzzle: I am taught that the lag order of VECM does not affect the cointegration rank because the lag order is for the differenced regressors. But, I see the contrary: I experimented with sample data of 4 variables, using lag orders between 1 and 12. At low lag orders the cointegration rank is low, but as I increase the lag length, cointegration rank goes up. Does anyone know of a paper or source discussing this issue? Any suggestions as to what to do in this situation? -->

<!-- Second puzzle: If I go by what information criteria indicate as an optimal lag length, establish the coinegration rank and then set up a VECM model, the residuals of the model are heteroscedastic and serially correlated. The info criteria based lag order is 2. To circumvent the issue, I have two options: -->

<!-- 1. Go back, increase the lag order and set up a new VECM and do diagnostic tests -->

<!-- 2. Just keep the cointegration rank as it is and increase the lag order until the residuals are homoscedastic and free of autocorrelation -->

<!-- What would be the optimal course of action? -->

<!-- *Answer* -->

<!-- This is a usual problem with the two steps procedure, where one selects first the lag, then the cointegration rank depending on the lag chose in the first step. -->

<!-- **Puzzle 1:** The claim that the lag order does not matter for the cointegration test depends on the type of test: -->

<!--  - VAR based test: if you use a Johansen test, the lag matters! -->

<!--  - Residual-based test (cf Engle-Granger, Phulips-Ouliaris), then yes, the lag order does not matter -->

<!-- **Puzzle 2:** My favourite approach is the simultaneous-selection approach, where you choose the lag and rank based on a simlutaneous criterion selection (This is implemented in package tsDyn in R for example, see http://www.inside-r.org/packages/cran/tsDyn/docs/rank.select). See references below. -->

<!-- Now what should you do if the ideal criterion selects a model where there is still some auto-correlation in errors? Great question, and I am not aware of papers tackling this directly. So one option would be to increase lags till you get a model with good residuals. Another option would be to stick with the AIC/BIC choice, and use heteroskedasticity and autocorrelation robust estimators (HAC) for any test you do (t-test, Granger causality, etc). Cheng an Philips (2009) show that their AIC/BIC lag/rank procedure is robust even in presence of auto-correlation (hence the name of semi-parametric). The issue is probably that few softwares will allow you do to this for all tests, in particular, IRF, usually computed with bootstrap, might not include a bootstrap scheme mimicking auto-correlation. -->

<!-- - Aznar A and Salvador M (2002). Selecting The Rank Of The Cointegration Space And The Form Of The Intercept Using An Information Criterion. Econometric Theory, *18*(04), pp. 926-947. . -->

<!-- - Cheng X and Phillips PCB (2009). Semiparametric cointegrating rank selection. Econometrics Journal , *12*(s1), pp. S83-S104. -->

<!-- *Question* -->

<!-- When preforming Johansen Cointegration test for 2 time series (the simple case) you need to decide the lag you want to use. Doing the test for different lags return different results: for some lag levels the null hypothesis can be rejected but for others it can't. -->

<!-- My question is what is the right method based on the input data to decide what lag I need to use when preforming the Johansen Test? -->

<!-- *Answer* -->

<!-- You are correct. The weakness of Johansen approach is that it is sensitive to the lag length. So, the lag length should be determined in a systematic manner. Following is the normal process used in the literature. -->

<!-- a. Choose maximum lag length "m" for VAR model. Usually, for annual data this is set to 1, for quarterly data this is set to 4, and for monthly data this is set to 12. -->

<!-- b. Run the VAR model in level. For example, if the data is monthly, run the VAR model for lag lengths 1,2, 3,....12. -->

<!-- c. Find the AIC (Akaike information criterion) and SIC (Schwarz information criterion) [ there are also other criteria  such as HQ (Hannan-Quin information criterion), FPE (Final prediction error criterion) but AIC and SIC are mostly used) for the VAR model for each lag length. Choose the lag length that minimizes AIC and SIC for the VAR model. Note that SIC and AIC may give conflicting results. -->

<!-- d. Finally, you MUST confirm that for the lag length you selected in step c, the residuals of the VAR model are not correlated [use Portmanteau Tests for autocorrelations]. You may have to modify the lag length, if there is the autocorrelation. Usually, beginners in time series econometrics tend to skip step d. -->

<!-- e. For the cointegration, the lag length is the lag length chosen from step d minus one (since we are running the model in first difference now, unlike in level when we used VAR to decide the lag length). -->

<!-- AIC or SBC could be used to help you decide what lag. The URCA package in R recommends selecting the lag having minimum AIC or SBC. -->

<!-- ## Question -->

<!-- I am testing for cointegration using the Johansen test. I have seen questions like how to interpret the test results, but when I am interpreting mine I have some doubts. In my results r = 3 since 4.10 < 10.49, so I cannot form a stationary series. It is the same for r = 2 and r = 1. But for r = 0, 86.12 > 59.14, so there is a stationary combination. -->

<!-- But r = 0 implies that there are zero cointegrating vectors. Does that mean that my data are not cointegrated and therefore I cannot build a VECM? -->

<!-- Please find my results below. -->

<!-- cointegration <- ca.jo(Canada, type="trace",ecdet="trend",spec="transitory") -->
<!-- > summary(cointegration) -->

<!-- ###################### -->
<!-- # Johansen-Procedure # -->
<!-- ###################### -->

<!-- Test type: trace statistic , with linear trend in cointegration -->

<!-- Eigenvalues (lambda): -->
<!-- [1]  4.483918e-01  2.323995e-01  1.313250e-01  4.877895e-02 -1.859499e-17 -->

<!-- Values of teststatistic and critical values of test: -->

<!--           test 10pct  5pct  1pct -->
<!-- r <= 3 |  4.10 10.49 12.25 16.26 -->
<!-- r <= 2 | 15.65 22.76 25.32 30.45 -->
<!-- r <= 1 | 37.33 39.06 42.44 48.45 -->
<!-- r = 0  | 86.12 59.14 62.99 70.05 -->

<!-- Eigenvectors, normalised to first column: -->
<!-- (These are the cointegration relations) -->

<!--                e.l1    prod.l1       rw.l1        U.l1    trend.l1 -->
<!-- e.l1      1.0000000  1.0000000  1.00000000  1.00000000  1.00000000 -->
<!-- prod.l1   0.3685667 -0.1582521  2.01545971  0.06122231 -0.09644538 -->
<!-- rw.l1    -0.1369713 -0.5035147 -0.08233586 -0.15589592 -0.47523051 -->
<!-- U.l1      3.2569951  2.4162383  2.98414327  1.57795960  1.54780259 -->
<!-- trend.l1 -0.1539863  0.1477376 -0.53596432 -0.20898570  0.16907450 -->

<!-- Weights W: -->
<!-- (This is the loading matrix) -->

<!--               e.l1     prod.l1       rw.l1        U.l1      trend.l1 -->
<!-- e.d     0.01520061  0.10989739  0.04306410 -0.01664954 -6.999563e-13 -->
<!-- prod.d  0.06282619  0.17899905 -0.05415524 -0.10283813 -5.525444e-12 -->
<!-- rw.d   -0.22958927  0.17308184 -0.03869293  0.06509098 -6.034107e-12 -->
<!-- U.d    -0.05230297 -0.08731406 -0.01833898 -0.03719022  1.367902e-12 -->

<!-- ## Answer -->

<!-- In Johansen cointegration test, the null hypothesis for the eigenvalue test is that there are r+1 -->

<!-- cointegration relations. -->

<!-- The test is therefore sequential: you test first for r=0 -->
<!-- , then r=1 -->

<!-- , etc. -->

<!-- The test concludes on the value of r -->
<!-- when the test fails to reject H0 for the first time. In your case, the test fails to reject the null hypothesis for the first time when r=1 -->

<!-- Therefore, you have one cointegration relationship. -->

<!-- ## Question -->

<!-- I've just started getting into cointegration testing in R using the "urca" and "tseries" packages last week and am still very confused about the different arguments, despite having read the manuals. This is of concern as my cointegration tests have so far yielded "no cointegration" results, when I know intuitively that my series should co-integrate (e.g. U.S. 10-year yield vs. U.S. 2-year yield, or XLE price vs. Brent 1st Futures) -->

<!-- I posted my results for the cointegration tests in a previous thread: Interpretation of results using Johansen and Engle-Granger 2-step Cointegration tests -->

<!-- Specifically for the Johansen Cointegration test, I read in another thread that -->

<!--     If you are really sure that there is a long term relationship in your data, then check that you are using the correct number of lags and appropriate dummy variables (constant, trend, seasonal dummies, etc) and then rerun the Johansen procedure again. -->

<!-- My questions then are: -->

<!-- (1) Lags: How do you select the optimal lags in the Johansen test? Unlike in the ADF test, I cannot let AIC select the lags for me. -->

<!-- (2) Type: When should you use trace vs. eigen? Some tutorials I've read stated that trace is preferred, but without any explanations on why it is so. -->

<!-- (3) ecdet: what does the argument ecdet refer to in the function ca.jo in package "urca"? The manual states that ecdet = Character, ‘none’ for no intercept in cointegration, ‘const’ for constant term in cointegration and ‘trend’ for trend variable in cointegration., but how do you decide which character fits? With the stationarity tests, I would plot the graphs to try and decipher if it's a random walk/with drift/with trend, but I am not sure if that makes sense for this test. -->

<!-- Here is a graph of U.S. 10-year vs. 2-year yields, if anyone would like to use it to elaborate: -->

<!-- The graph is inside: [./literatures/temporary/FU...jpg] -->

<!-- ## Answer -->

<!-- The lag selection for cointegration test is the same as selecting lags for VAR model, since cointegration is a actually a special feature of VAR model. Use VARselect to choose number of lags. -->

<!-- The two statistics test the same thing and are constructed from the same eigenvalues of a certain matrix. For practical purposes there are no differences between these two. -->

<!-- Cointegration means that the linear combination of unit root processes is stationary process. It is usually assumed that this stationary process has zero mean. However it is entirely possible that it has a non-zero mean and there is a trend added to the process. In the case of trend and two unit root processes this means that the difference ?? has a trend, which means that the two processes are pushed apart over time. Judging from your graph it would be difficult to argue if this is really the case. -->

# Bibliography
